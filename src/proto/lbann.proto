syntax = "proto3";

package lbann_data;

message LbannPB {
  DataReader data_reader = 1;
  Model model = 2;
  Optimizer optimizer = 3;
  MotifDefinitions motif_definitions = 4;
}

//========================================================================
// DataReaders
//========================================================================
message DataReader {
  int64 max_par_io_size = 1;
  repeated Reader reader = 2;
}

message Reader {
  string name = 1; //mnist, nci, nci_regression, numpy, imagenet, synthetic, merge_samples
  string role = 3; //train, validation, test
  bool shuffle = 4;
  string data_filedir = 5;
  string data_local_filedir = 50; //to support data_store
  string data_filename = 6;
  string label_filename = 7;
  double validation_percent = 9;
  int64 absolute_sample_count = 11;
  int64 first_n = 200;
  double percent_of_data_to_use = 12;
  //for GAN model
  bool gan_labelling = 201;
  int32 gan_label_value = 202;
  ImagePreprocessor image_preprocessor = 13;

  //------------------ start of only for jag_conduit -----------------------
  repeated string jag_scalar_keys = 91;
  repeated string jag_input_keys = 92;
  message JagKeyPrefixFilter {
    string key_prefix = 1;
    uint32 min_len = 2;
  }
  repeated string jag_scalar_filters = 93;
  repeated JagKeyPrefixFilter jag_scalar_prefix_filters = 94;
  repeated string jag_input_filters = 95;
  repeated JagKeyPrefixFilter jag_input_prefix_filters = 96;
  repeated int32 independent = 97;
  repeated int32 dependent = 98;
  int32 max_files_to_load = 1000;

  // for jag_conduit_hdf5
  bool use_scalars = 1001;
  bool use_images = 1002;
  bool use_inputs = 1003;
  //------------------  end of only for jag_conduit  -----------------------

  int32 num_labels = 99; //for imagenet and synthetic
  int64 num_samples = 100; //only for synthetic
  string synth_dimensions = 101; //only for synthetic
  //csv attributes
  string separator = 102;
  int32 skip_cols = 103;
  int32 skip_rows = 104;
  bool has_header = 105;
  int32 label_col = 106;
  int32 response_col = 107;
  bool disable_labels = 108;
  bool disable_responses = 109;
  string format = 110; // numpy, csv
  string data_file_pattern = 111;
  int64 num_neighbors = 112; // pilot2_molecular_reader
  int64 max_neighborhood = 113; // pilot2_molecular_reader
  int32 num_image_srcs = 114; // data_reader_multi_images

  //------------- start of only for partitioned data sets ------------------
  bool is_partitioned = 300; 
  double partition_overlap = 301;
  int32 partition_mode = 302;  
       // 1 - share a portion of your data with two neighbors;
       // 2 - there's a set of overlap indices that are common to all models
  //------------- end of only for partitioned data sets ------------------
}

message ImagePreprocessor {
  string name = 1;
  bool disable = 2;
  int32 raw_width = 3;
  int32 raw_height = 4;

  message Cropper {
    string name = 1;
    bool disable = 2;
    bool crop_randomly = 3;
    uint32 crop_width = 4;
    uint32 crop_height = 5;
    int32 resized_width = 6;
    int32 resized_height = 7;
    bool adaptive_interpolation = 8;
  }

  message Resizer {
    string name = 1;
    bool disable = 2;
    int32 resized_width = 3;
    int32 resized_height = 4;
    bool adaptive_interpolation = 5;
  }

  message Augmenter {
    string name = 1;
    bool disable = 2;
    bool horizontal_flip = 3;
    bool vertical_flip = 4;
    double rotation = 5;
    double horizontal_shift = 6;
    double vertical_shift = 7;
    double shear_range = 8;
  }

  message Decolorizer {
    string name = 1;
    bool disable = 2;
    bool pick_1ch = 3;
  }

  message Colorizer {
    string name = 1;
    bool disable = 2;
  }

  message Normalizer {
    string name = 1;
    bool disable = 2;
    bool scale = 3;
    bool subtract_mean = 4;
    bool unit_variance = 5;
    bool z_score = 6;
  }

  message Subtractor {
    string name = 1;
    bool disable = 2;
    string image_to_sub = 3;
    string image_to_div = 4;
    repeated float channel_mean = 5 [packed = true];
    repeated float channel_stddev = 6 [packed = true];
  }

  message PatchExtractor {
    string name = 1;
    bool disable = 2;
    uint32 patch_width = 3;
    uint32 patch_height = 4;
    uint32 patch_gap = 5; // gap between patches
    uint32 patch_jitter = 6; // max jittering amount for patch positions
    uint32 centering_mode = 7; // center patch positioning mode
    uint32 ca_correction_mode = 8; // chromatic abberation correction mode
  }

  message Noiser {
    string name = 1;
    bool disable = 2;
    float factor = 3;
  }

  Cropper cropper = 5;
  Resizer resizer = 34;
  Augmenter augmenter = 6;
  Decolorizer decolorizer = 7;
  Colorizer colorizer = 8;
  Subtractor subtractor = 9;
  Normalizer normalizer = 10;
  Noiser noiser = 11;
  PatchExtractor patch_extractor = 12;

  int32 early_normalization = 33; // for data_reader_jag only
}

// TODO: wrap El::Mat based normalization into a generic preprocessor
message GenericPreprocessor {
  string name = 1;
  bool disable = 2;

  message Normalizer {
    string name = 1;
    bool disable = 2;
    bool scale = 3;
    bool subtract_mean = 4;
    bool unit_variance = 5;
    bool z_score = 6;
  }

  Normalizer normalizer = 3;
}

//========================================================================
// Model
//========================================================================

message Model {
  string name = 1; //sequential_model, dag_model, greedy_layerwise_autoencoder, siamese_model
  ObjectiveFunction objective_function = 2;
  repeated Metric metric = 5;
  string data_layout = 6;

  int64 mini_batch_size = 12;
  int64 num_epochs = 4;
  int64 super_steps = 121; //multiple steps/epochs currently use in GAN
  int64 num_batches = 122; //multiple batches/sub epoch
  int64 block_size = 50;
  int64 procs_per_model = 51;
  int64 num_gpus = 53; //has no effect
  int64 evaluation_frequency = 54;
  int64 num_parallel_readers = 100;

  bool disable_cuda = 8;

  repeated Layer layer = 10;

  repeated Weights weights = 11;

  // checknan, debug, dump_activations, etc;
  // for complete list, see: lbann/include/lbann/callbacks
  repeated Callback callback = 20;

  int64 random_seed = 30;
  // If true, models will have their model rank mixed into their random seed.
  bool random_init_models_differently = 31;

  // Siamese model parameters
  message Siamese {
    uint32 num_heads = 1;
  }
  Siamese siamese = 37;
}

//========================================================================
// Objective function
//========================================================================

message ObjectiveFunction {
  repeated MeanSquaredError mean_squared_error = 10;
  repeated MeanAbsoluteDeviation mean_absolute_deviation = 11;
  repeated MeanAbsoluteError mean_absolute_error = 24;
  repeated CrossEntropy cross_entropy = 12;
  repeated BinaryCrossEntropy binary_cross_entropy = 13;
  repeated CrossEntropyWithUncertainty cross_entropy_with_uncertainty = 14;
  repeated GeomNegLogLike geom_negloglike = 15;
  repeated PoissonNegLogLike poisson_negloglike = 16;
  repeated PolyaNegLogLike polya_negloglike = 17;
  repeated L1WeightRegularization l1_weight_regularization = 20;
  repeated L2WeightRegularization l2_weight_regularization = 21;
  repeated GroupLassoWeightRegularization group_lasso_weight_regularization = 22;
  repeated LayerTerm layer_term = 25;
}

message MeanSquaredError {
  double scale_factor = 1;
}

message MeanAbsoluteDeviation {
  double scale_factor = 1;
}

message MeanAbsoluteError {
  double scale_factor = 1;
}

message CrossEntropy {
  double scale_factor = 1;
}


message BinaryCrossEntropy {
  double scale_factor = 1;
}

message CrossEntropyWithUncertainty {
  double scale_factor = 1;
}

message GeomNegLogLike {
  double scale_factor = 1;
}

message PoissonNegLogLike {
  double scale_factor = 1;
}

message PolyaNegLogLike {
  double scale_factor = 1;
}

message L1WeightRegularization {
  double scale_factor = 1;
}

message L2WeightRegularization {
  double scale_factor = 1;
}

message GroupLassoWeightRegularization {
  double scale_factor = 1;
}

message LayerTerm {
  double scale_factor = 1;
  string layer = 2;
}

//========================================================================
// Metrics
//========================================================================

message Metric {
  // a Metric should contain exactly one of the following
  CategoricalAccuracy categorical_accuracy = 1;
  TopKCategoricalAccuracy top_k_categorical_accuracy = 2;
  MeanSquaredError mean_squared_error = 3;
  MeanAbsoluteDeviation mean_absolute_deviation = 4;
  MeanAbsoluteError mean_absolute_error = 6;
  PearsonCorrelation pearson_correlation = 5;
  R2 r2 = 7;
  BooleanAccuracy boolean_accuracy = 8;
  BooleanFalsePositives boolean_false_positives = 9;
  BooleanFalseNegatives boolean_false_negatives = 10;
  LayerMetric layer_metric = 11;
}

message CategoricalAccuracy {
}

// Already defined as an objective function
// message MeanSquaredError {}

message TopKCategoricalAccuracy {
  int64 k = 1;
  int64 top_k = 2; //deprecated
}

message PearsonCorrelation {
}

message R2 {
}

message BooleanAccuracy {
}

message BooleanFalsePositives {
}

message BooleanFalseNegatives {
}

message LayerMetric {
  string layer = 1;
  string name = 2;
  string unit = 3;
}

//========================================================================
// Optimizers
//========================================================================
message Optimizer {
  // An Optimizer should contain exactly one of the following
  // (this may or may not be properly checked for in proto_common.cpp)
  Adagrad adagrad = 1;
  Rmsprop rmsprop = 2;
  Adam adam = 3;
  HypergradientAdam hypergradient_adam = 4;
  Sgd sgd = 5;
}

message Adagrad {
  double learn_rate = 1;
  double eps = 2;  //default: 1e-8
}

message Adam {
  double learn_rate = 1;
  double beta1 = 6;    //default: 0.9
  double beta2 = 7;    //default: 0.99
  double eps = 8;      //default: 1e-8
}

message HypergradientAdam {
  double init_learning_rate = 1;
  double hyper_learning_rate = 2; //default: 1e-7
  double beta1 = 6;    //default: 0.9
  double beta2 = 7;    //default: 0.99
  double eps = 8;      //default: 1e-8
}

message Rmsprop {
  double learn_rate = 1;
  double decay_rate = 2;
  double eps = 3; //default: 1e-8
}

message Sgd {
  double learn_rate = 1;
  double momentum = 2;     //default: 0
  double decay_rate = 3;   //default: 0
  bool nesterov = 4;       //default: false
}


//========================================================================
// Callbacks
//========================================================================
message Callback {
   // a Callback should contain exactly one of the following
   CallbackPrint print = 1;
   CallbackTimer timer = 2;
   CallbackSummary summary = 3;
   CallbackDumpWeights dump_weights = 4;
   CallbackDumpActivations dump_activations = 5;
   CallbackDumpErrorSignals dump_error_signals = 35;
   CallbackDumpGradients dump_gradients = 6;
   CallbackDumpMBIndices dump_mb_indices = 7;
   CallbackDispIOStats disp_io_stats = 8;
   CallbackImComm imcomm = 9;
   CallbackSaveImages save_images = 10;
   CallbackDebug debug = 11;
   CallbackAdaptiveLearningRate adaptive_learning_rate = 12;
   CallbackStepLearningRate step_learning_rate = 13;
   CallbackCustomLearningRate custom_learning_rate = 14;
   CallbackCheckSmall check_small = 15;
   CallbackCheckNaN check_nan = 16;
   CallbackCheckDataset check_dataset = 17;
   CallbackHang hang = 18;
   CallbackDropFixedLearningRate drop_fixed_learning_rate = 19;
   CallbackLinearGrowthLearningRate linear_growth_learning_rate = 20;
   CallbackProfiler profiler = 21;
   CallbackStepMinibatch step_minibatch = 22;
   CallbackGradientCheck gradient_check = 23;
   CallbackLTFB ltfb = 24;
   CallbackDebugIO debug_io = 25;
   CallbackMinibatchSchedule minibatch_schedule = 26;
   CallbackOptimizerwiseAdaptiveLearningRate optimizerwise_adaptive_learning_rate = 27;
   CallbackCheckpoint checkpoint = 28;
   CallbackSaveModel save_model = 29;
   CallbackPolyLearningRate poly_learning_rate = 30;
   CallbackReplaceWeights replace_weights = 31;
   CallbackGPUMemoryUsage gpu_memory_usage = 32;
   CallbackSyncLayers sync_layers = 33;
   CallbackSyncSelected sync_selected = 34;
}

message CallbackLTFB {
  int64 round_size = 1; 
  bool increasing_metric_mode = 2; //Expectation for a good tournament metric: increasing (true) is default 
  string eval_metrics = 3; //eval metrics to use for tournament, at least 1 metric has to be provided
  string weights_tosend = 4; //list of weights to transfer between model, default is all weights (classic LTFB)
}

message CallbackStepLearningRate {
  string weights = 1; //default: all weights
  int64 step = 2;
  double amt = 3;
}

message CallbackCustomLearningRate {
  //don't know how to support this, since it takes an std::function as an argument
}

message CallbackAdaptiveLearningRate {
  string weights = 1; //default: all weights
  int64 patience = 2;
  double amt = 3;
}

message CallbackSaveImages {
  string image_dir = 1;
  string layer_names = 2; //layer(s) at which to save images  e.g., "input, reconstruction"
  string extension = 3;
}

message CallbackPrint {
  int64 interval = 1; //default in lbann_callback_print.hpp is 1
}

message CallbackProfiler {
  bool sync = 1;
}

message CallbackTimer {
}

message CallbackSummary {
  string dir = 1; //directory for the lbann_summary
  int64 batch_interval = 2; //default in lbann_callback_summary.hpp is 1
  int64 mat_interval = 3; //default in lbann_callback_summary.hpp is 25
}

message CallbackDumpWeights {
  string basename = 1;
}

message CallbackDumpActivations {
  string basename = 1;
  int64 interval = 2;
  string layer_names = 3; //layer(s) at which to dump activations e.g., "relu1 relu4 relu12"

}

message CallbackDumpErrorSignals {
  string basename = 1;
}

message CallbackDumpGradients {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDumpMBIndices {
  string basename = 1;
  int64 interval = 2;
}

message CallbackDispIOStats {
  string layers = 1; //e.g: "2 4 5"; use "10000" to apply to all layers
}

message CallbackImComm {
  string intermodel_comm_method = 1;
  bool all_optimizers = 2;
}

message CallbackDebug {
  string phase = 1; //should be called "modes"
}

message CallbackDebugIO {
  string phase = 1;
  int32 lvl = 2;
}

message CallbackCheckSmall {
}

message CallbackCheckNaN {
}

message CallbackCheckDataset {
}

message CallbackHang {
  int64 rank = 1;
}

message CallbackDropFixedLearningRate {
  string weights = 1;
  repeated int64 drop_epoch = 2;
  double amt = 3;
}

message CallbackLinearGrowthLearningRate {
  string weights = 1;
  double target = 2;
  int64 num_epochs = 3;
  int64 delay = 4;
}

message CallbackPolyLearningRate {
  string weights = 1;
  double power = 2;
  uint64 num_epochs = 3;
  uint64 max_iter = 4;
}

message CallbackStepMinibatch {
  int64 starting_mbsize = 1;
  int64 step = 2;
  int64 ramp_time = 3;
}

message MinibatchScheduleStep {
  int64 epoch = 1;
  int64 mbsize = 2;
  double lr = 3;
  int64 ramp_time = 4;
}

message CallbackOptimizerwiseAdaptiveLearningRate {
  string weights = 1;
  double scale = 2;
}

message CallbackMinibatchSchedule {
  int64 starting_mbsize = 1;
  repeated MinibatchScheduleStep step = 2;
}

message CallbackGradientCheck {
  double step_size = 1;
  bool verbose = 2;
  bool fail_on_error = 3;
}


message CallbackCheckpoint {
  string checkpoint_dir = 1;
  int64 checkpoint_epochs = 2;
  int64 checkpoint_steps = 3;
  double checkpoint_secs = 4;
  string per_rank_dir = 5;
  int64 ckpt_dist_epochs = 6;
  int64 ckpt_dist_steps = 7;
}


message CallbackSaveModel {
  string dir = 1;
  string extension = 2;
}

message CallbackReplaceWeights {
  string source_layers = 1; //set of layers to copy weights from
  string destination_layers = 2;  //set of layers to copy weights to
  int64 batch_interval = 3;
}
message CallbackGPUMemoryUsage {
}

message CallbackSyncLayers {
  bool sync_gpus = 1;
  bool sync_mpi = 2;
  bool only_input = 3;
}

message CallbackSyncSelected {
  message LayerToSync {
    enum PropDirection {
      Both = 0;
      Forward = 1;
      Backward = 2;
    }
    string name = 1; // name of the layer to synchronize
    PropDirection prop = 2; // propagation setep to synchronize
  }

  message CudaProfilerSetup {
    enum OutputMode {
      KeyValuePair = 0;
      CSV = 1;
    }
    bool no_init = 1;
    string config_file = 2;
    string output_dir = 3;
    OutputMode output_mode = 4;
  }

  bool async_gpus = 1;
  bool async_mpi = 2;
  repeated LayerToSync layer_to_sync = 3;
  CudaProfilerSetup cuda_profiler_setup = 4;
}

//========================================================================
// Weights
//========================================================================

message Weights {

  string name = 1;
  Optimizer optimizer = 2;

  ConstantInitializer constant_initializer = 20;
  ValueInitializer value_initializer = 21;
  UniformInitializer uniform_initializer = 22;
  NormalInitializer normal_initializer = 23;
  GlorotNormalInitializer glorot_normal_initializer = 24;
  GlorotUniformInitializer glorot_uniform_initializer = 25;
  HeNormalInitializer he_normal_initializer = 26;
  HeUniformInitializer he_uniform_initializer = 27;
  LeCunNormalInitializer lecun_normal_initializer = 28;
  LeCunUniformInitializer lecun_uniform_initializer = 29;

}

// Weight initializers
message ConstantInitializer {
  double value = 1;
}
message ValueInitializer {
  string values = 1;
}
message UniformInitializer {
  double min = 1;
  double max = 2;
}
message NormalInitializer {
  double mean = 1;
  double standard_deviation = 2;
}
message GlorotNormalInitializer {}
message GlorotUniformInitializer {}
message HeNormalInitializer {}
message HeUniformInitializer {}
message LeCunNormalInitializer {}
message LeCunUniformInitializer {}

//note: I'd like to put this enum inside of Layer, but if I do the enum values
//      become, e.g, Layer_Imcomm_EXCLUDE, which is just ugly
enum Imcomm {
  DEFAULT = 0; //add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  EXCLUDE = 1; //*do not* add Layer to Imcomm callback if all_learning_layers = true in
               //the CallbackImComm
  INCLUDE = 2;  //add Layer to Imcomm callback regardless of whether all_learning_layers
                //in the CallbackImComm is set to true or false
}

// Weight data for exporting
message WeightsShape {
  repeated int64 dim = 1 [packed = true];
}
message WeightsData {
  WeightsShape shape = 5;
  string name = 1;
  int64 height = 2;
  int64 width = 3;
  //@todo assume float above, add other datatype
  repeated float data = 4 [packed=true];

  Imcomm imcomm = 55;
}

//========================================================================
// MotifDefinitions
//========================================================================

message MotifDefinitions {
  repeated Motif motif = 1;
}

message Motif {
  string name = 1;
  repeated Layer layer = 2;
}

//========================================================================
// Layers
//========================================================================

message Layer {
   string name = 50;
   string parents = 151;
   string children = 152;
   string data_layout = 52;
   string device_allocation = 55;
   string weights = 54;
   bool num_neurons_from_data_reader = 53;
   bool freeze = 5;

   repeated WeightsData weights_data = 153;
   string top = 154;
   string bottom = 155;
   string type = 156;

   // a Layer should contain exactly one of the following
   // (this may or may not be properly checked for in proto_common.cpp)
   //
   // @todo: this should be done better using oneof:
   //   oneof a_layer {
   //       Reshape reshape = 306
   //       Pooling pooling = 12;
   //       ...
   //   }
   //
   //

   // motif layer
   MotifLayer motif_layer = 4;

   // Input layers
   Input input = 2;

   // Transform layers
   Reshape reshape = 306;
   Pooling pooling = 12;
   Concatenation concatenation = 300;
   Slice slice = 301;
   Split split = 302;
   Sum sum = 303;
   WeightedSum weighted_sum = 323;
   Unpooling unpooling = 304;
   Hadamard hadamard = 308;
   Constant constant = 309;
   Zero zero = 315;
   Reduction reduction = 310;
   Evaluation evaluation = 311;
   Gaussian gaussian = 312;
   Bernoulli bernoulli = 313;
   Uniform uniform = 314;
   Crop crop = 316;
   CategoricalRandom categorical_random = 317;
   DiscreteRandom discrete_random = 318;
   Dummy dummy = 319;
   StopGradient stop_gradient = 320;
   Max max = 321;
   Min min = 322;
   InTopK in_top_k = 324;
   Sort sort = 325;
   WeightsLayer weights_layer = 326;

   // Learning layers
   FullyConnected fully_connected = 11;
   Convolution convolution = 13;
   Deconvolution deconvolution = 305;

   // Loss layers
   CrossEntropy cross_entropy = 60;
   MeanSquaredError mean_squared_error = 61;
   TopKCategoricalAccuracy top_k_categorical_accuracy = 62;
   L2Norm2 l2_norm2 = 63;

   // Math layers
   Not not = 401;
   Abs abs = 402;
   Negative negative = 403;
   Sign sign = 404;
   Round round = 405;
   Ceil ceil = 406;
   Floor floor = 407;
   Reciprocal reciprocal = 408;
   Square square = 409;
   Sqrt sqrt = 410;
   Rsqrt rsqrt = 411;
   Exp exp = 412;
   Expm1 expm1 = 413;
   Log log = 414;
   Log1p log1p = 415;
   Cos cos = 416;
   Sin sin = 417;
   Tan tan = 418;
   Acos acos = 419;
   Asin asin = 420;
   Atan atan = 421;
   Cosh cosh = 422;
   Sinh sinh = 423;
   Tanh tanh = 424;
   Acosh acosh = 425;
   Asinh asinh = 426;
   Atanh atanh = 427;
   
   // Target Layers
   Target target = 18;
   TargetReconstruction reconstruction = 22;

   // Regularization Layers
   BatchNormalization batch_normalization = 19;
   LocalResponseNormalization local_response_normalization = 20;
   Dropout dropout = 21;
   SeluDropout selu_dropout = 229;

   // Activation Layers
   Softmax softmax = 200;
   LogSoftmax logsoftmax = 203;
   ELU elu = 30;
   Identity identity = 31;
   LeakyRelu leaky_relu = 32;
   Relu relu = 33;
   Sigmoid sigmoid = 34;
   SmoothRelu smooth_relu = 35;
   Softplus softplus = 36;
   Selu selu = 37;
   BentIdentity bent_identity = 40;
   Swish swish = 42;
   Power power = 43;
   Sigmoid_Binary_Cross_Entropy_With_Logits bce_with_logits = 47;

}
///////////////////////
// MotifLayer //
///////////////////////
message MotifLayer {
  string motif_id = 1;
  repeated string variable = 2;
}

///////////////////////
// Math Layers       //
///////////////////////
message Not {}
message Abs {}
message Negative {}
message Sign {}
message Round {}
message Ceil {}
message Floor {}
message Reciprocal {}
message Square {}
message Sqrt {}
message Rsqrt {}
message Exp {}
message Expm1 {}
message Log {}
message Log1p {}
message Cos {}
message Sin {}
message Tan {}
message Acos {}
message Asin {}
message Atan {}
message Cosh {}
message Sinh {}
message Tanh {}
message Acosh {}
message Asinh {}
message Atanh {}

///////////////////////
// Activation Layers //
///////////////////////
message ELU {
  double alpha = 2; //default: 1.0; must be >= 0
}

message Identity {
}

message LeakyRelu {
  double leak = 2; //default: 0.01
}

message Relu {
}

message Sigmoid {
}

message SmoothRelu {
}

message Softplus {
}

message BentIdentity {
}

message Swish {
}

message Selu {
  double alpha = 2; //default: 1.6732632423543772848170429916717
  double scale = 3; //default: 1.0507009873554804934193349852946
}

message Softmax {
}

message LogSoftmax {
}

message Power {
  double exponent = 1;
}

message Sigmoid_Binary_Cross_Entropy_With_Logits {
  int32 true_label = 1;
}

///////////////////////
// Loss Layers //
///////////////////////
message L2Norm2 {}

///////////////////////////
// Regularization Layers //
///////////////////////////
message BatchNormalization {
  double decay = 1;          //default: 0.9
  double scale_init = 2;     //default: 1.0
  double bias_init = 3;      //default: 0.0
  double epsilon = 4;        //default: 1e-5
  bool global_stats = 5;     //default: false
}

message SeluDropout {
  double keep_prob = 2; //default: 0.95
  double alpha = 3;     //default: 1.6732632423543772848170429916717
  double scale = 4;     //default: 1.0507009873554804934193349852946
}

message LocalResponseNormalization {
  int64 window_width = 4;
  double lrn_alpha = 5;
  double lrn_beta = 6;
  double lrn_k = 7;
}

message Dropout {
  double keep_prob = 2;  //default: 0.5
}

//////////////////
// Input Layers //
//////////////////
message Input {
  bool data_set_per_model = 1;  //default: false
  string io_buffer = 2;
  string target_mode = 3;
}

/// @todo Remove when possible
message RepeatedInput {
  bool data_set_per_model = 1;  //default: false
  int64 num_steps = 2;
  string target_mode = 3;
}

//////////////////////
// transform Layers //
//////////////////////
message Reshape {
  int64 num_dims = 1;
  string dims = 2; //should be space-separated list of ints, e.g, "2 6 7"
  bool reshape_to_flattened_conv_format = 3;
}

message Pooling {
  int64 num_dims = 1;

  bool has_vectors = 2;

  //these are used if has_vectors = true
  string pool_dims = 4; //should be space-separated list, e.g, "2 2 3"
  string pool_pads = 5; //should be space-separated list, e.g, "2 2 3"
  string pool_strides = 6; //should be space-separated list, e.g, "2 2 3"

  //these are used if has_vectors = false
  int64 pool_dims_i = 10;
  int64 pool_pads_i = 11;
  int64 pool_strides_i = 12;

  //pool_mode should be one of: max, average, average_no_pad
  //see: lbann/include/lbann/lbann_base.hpp
  string pool_mode = 7;
}

message Unpooling {
  int64 num_dims = 1;
  string pooling_layer = 13; //should be name of the pooling layer
}


message Concatenation {
  string parents = 1; //TODO: this doesn't do anything and should be removed
  int64 concatenation_axis = 2;
}

message Slice {
  int64 slice_axis = 2;
  string slice_points = 3; //should be space-separated list of ints, e.g, "2 6 7"
}

message Split {
}

message Sum {
}

message WeightedSum {
  string scaling_factors = 1; //should be a space-separated list of doubles, e.g. "1.0 2.0 -1.0"
}

message Hadamard {
}

message Constant {
  double value=1;
  string num_neurons=2;
}


message Zero {
  bool first_half=1; //default: true
  bool second_half=2; //default: true
}

message Reduction {
  string mode=1; //"sum" or "average"
}

message Evaluation {
}

message Gaussian {
  double mean = 1;
  double stdev = 2;
  string neuron_dims = 3;
}

message Bernoulli {
  double prob = 1;
  string neuron_dims = 2;
}

message Uniform {
  double min = 1;
  double max = 2;
  string neuron_dims = 3;
}


message Crop {
  string dims = 3;
}

message CategoricalRandom {
}

message DiscreteRandom {
  string values = 1;
  string dims = 2;
}

message Dummy {
}

message StopGradient {
}

message Max {
}

message Min {
}

message InTopK {
  int64 k = 1;
}

message Sort {
  bool descending = 1;
}

message WeightsLayer {
  string dims = 1;
}

/////////////////////
// learning Layers //
/////////////////////
message FullyConnected {
  int64 num_neurons = 1;
  string weight_initialization = 2;    //DEPRECATED
  bool has_bias = 3;                   //default: true
  double bias_initial_value = 4;       //default: 0
  double l2_regularization_factor = 5; //default: 0
  double group_lasso_regularization_factor = 6; //default: 0
  bool transpose = 7;
  bool num_neurons_is_num_labels = 8;
}

message Convolution {
  int64 num_dims = 1;
  int64 num_output_channels = 4;

  bool has_vectors = 2;

  // these are used if has_vector = true
  string conv_dims = 5; //should be space-separated list, e.g, "2 2 3"
  string conv_pads = 6;  //should be space-separated list, e.g, "2 2 3"
  string conv_strides = 7; //should be space-separated list, e.g, "2 2 3"

  // these are used if has_vector = false
  int64 conv_dims_i = 50;
  int64 conv_pads_i = 60;
  int64 conv_strides_i = 70;

  string weight_initialization = 9;     //DEPRECATED
  bool has_bias = 10;                   //default: true
  double bias_initial_value = 11;       //default: 0
  double l2_regularization_factor = 12; //default: 0
}

message Deconvolution {
  int64 num_dims = 1;
  int64 num_output_channels = 4;

  bool has_vectors = 2;

  // these are used if has_vector = true
  string conv_dims = 5; //should be space-separated list, e.g, "2 2 3"
  string conv_pads = 6;  //should be space-separated list, e.g, "2 2 3"
  string conv_strides = 7; //should be space-separated list, e.g, "2 2 3"

  // these are used if has_vector = false
  int64 conv_dims_i = 50;
  int64 conv_pads_i = 60;
  int64 conv_strides_i = 70;

  string weight_initialization = 9;     //DEPRECATED
  bool has_bias = 10;                   //default: true
  double bias_initial_value = 11;       //default: 0
  double l2_regularization_factor = 12; //default: 0
}

///////////////////
// Target Layers //
///////////////////
message Target {
  string paired_input_layer = 1;
  bool shared_data_reader = 2;
  bool for_regression = 3; //default: false
  string io_buffer = 4;
}

message TargetReconstruction {
}
