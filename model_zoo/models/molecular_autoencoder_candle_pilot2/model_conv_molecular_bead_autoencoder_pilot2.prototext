model {
  mini_batch_size: 32
  block_size: 256
  num_epochs: 4
  num_parallel_readers: 1
  procs_per_trainer: 0

  ##############################################
  # Objective function
  ##############################################

  objective_function {
    layer_term { layer: "mean_squared_error" }
    l2_weight_regularization {
      scale_factor: 0.0005
    }
  }

  ##############################################
  # Callbacks
  ##############################################

  callback { print {} }
  callback { timer {} }

  ##############################################
  # Layers
  ##############################################

  #######
  # INPUT
  #######

  layer {
    name: "input"
    children: "data label"
    data_layout: "data_parallel"
    input {
      io_buffer: "partitioned"
    }
  }
  layer {
    parents: "input"
    name: "data"
    data_layout: "data_parallel"
    reshape {
      dims: "1 -1" # Reshape to 1xX tensor
    }
  }
  layer {
    parents: "input"
    name: "label"
    data_layout: "data_parallel"
    dummy {}
  }

  ######################
  # Encoder
  ######################

  # encode1
  layer {
    parents: "data"
    name: "encode1_conv"
    data_layout: "data_parallel"
    convolution {
      num_dims: 1
      num_output_channels: 512
      conv_dims: "20"
      conv_pads: "0"
      conv_strides: "20"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "encode1_conv"
    name: "encode1_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "encode1_bn"
    name: "encode1"
    data_layout: "data_parallel"
    relu {}
  }

  # encode2
  layer {
    parents: "encode1"
    name: "encode2_conv"
    data_layout: "data_parallel"
    convolution {
      num_dims: 1
      num_output_channels: 1024
      conv_dims: "12"
      conv_pads: "0"
      conv_strides: "12"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "encode2_conv"
    name: "encode2_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "encode2_bn"
    name: "encode2"
    data_layout: "data_parallel"
    relu {}
  }

  # encode3
  layer {
    parents: "encode2"
    name: "encode3_fc"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 12
      has_bias: true
    }
  }
  layer {
    parents: "encode3_fc"
    name: "encode3_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "encode3_bn"
    name: "encode3"
    data_layout: "data_parallel"
    relu {
    }
  }

  ######################
  # Decoder
  ######################

  # decode3
  weights {
    name: "decode3_fc_matrix"
    glorot_uniform_initializer {}
  }
  layer {
    parents: "encode3"
    name: "decode3_fc"
    weights: "decode3_fc_matrix"
    hint_layer: "encode2"
    data_layout: "data_parallel"
    fully_connected {
      has_bias: true
    }
  }
  layer {
    parents: "decode3_fc"
    name: "decode3_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "decode3_bn"
    name: "decode3"
    data_layout: "data_parallel"
    relu {}
  }

  # decode2
  layer {
    parents: "decode3"
    name: "decode2_deconv"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 1
      num_output_channels: 512
      conv_dims: "12"
      conv_pads: "0"
      conv_strides: "12"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "decode2_deconv"
    name: "decode2_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "decode2_bn"
    name: "decode2"
    data_layout: "data_parallel"
    relu {}
  }

  # decode1
  layer {
    parents: "decode2"
    name: "decode1_deconv"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 1
      num_output_channels: 1
      conv_dims: "20"
      conv_pads: "0"
      conv_strides: "20"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "decode1_deconv"
    name: "decode1_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "decode1_bn"
    name: "decode1"
    data_layout: "data_parallel"
    elu {}
  }

  ######################
  # Reconstruction
  ######################

  layer {
    parents: "decode1 data"
    name: "mean_squared_error"
    data_layout: "data_parallel"
    mean_squared_error {}
  }

}
