<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>LBANN: Layers</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">LBANN
   &#160;<span id="projectnumber">@LBANN_MAJOR_VERSION@.@LBANN_MINOR_VERSION@</span>
   </div>
   <div id="projectbrief">LivermoreBigArtificialNeuralNetworkToolkit</div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('layers.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Layers </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Detailed information for layer types and options within these types can be found in this section. The layers are <a class="el" href="layers.html#learning">Learning</a>, <a class="el" href="layers.html#regularizer">Regularizer</a>, <a class="el" href="layers.html#transform">Transform</a>, <a class="el" href="layers.html#activation">Activation</a>, and <a class="el" href="layers.html#i_o">IO</a>.</p>
<h1><a class="anchor" id="learning"></a>
Learning</h1>
<h2><a class="anchor" id="conv"></a>
Convolution</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1convolution__layer.html" title="Convolution layer. ">lbann::convolution_layer</a></p>
<h2><a class="anchor" id="deconv"></a>
Deconvolution</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1deconvolution__layer.html" title="Deconvolution layer. ">lbann::deconvolution_layer</a></p>
<h2><a class="anchor" id="ip"></a>
Fully Connected</h2>
<p>Fully-connected layer. This layer applies an affine transformation.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1fully__connected__layer.html">lbann::fully_connected_layer</a></p>
<h1><a class="anchor" id="regularizer"></a>
Regularizer</h1>
<h2><a class="anchor" id="batchNorm"></a>
Batch Normalization</h2>
<p>Batch normalization layer. Each input channel is normalized across the mini-batch to have zero mean and unit standard deviation. Learned scaling factors and biases are then applied. See: Sergey Ioffe and Christian Szegedy. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ICML 2015. This uses the standard approach of maintaining the running mean and standard deviation (with exponential decay) for use at test time. See: <a href="https://cthorey.github.io/backpropagation/">https://cthorey.github.io/backpropagation/</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1batch__normalization.html">lbann::batch_normalization</a></p>
<h2><a class="anchor" id="dropout"></a>
Dropout</h2>
<p>Dropout layer: Probabilistically drop layer outputs. See: Srivastava, Nitish, et al. "Dropout: a simple way to prevent
  neural networks from overfitting." Journal of Machine Learning Research 15.1 (2014). The weights are multiplied by 1/(keep probability) at training time, as discussed in section 10 of the paper. Keep probabilities of 0.5 for fully-connected layers and 0.8 for input layers are good starting points.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1dropout.html">lbann::dropout</a></p>
<h2><a class="anchor" id="selu_dropout"></a>
Selu Dropout</h2>
<p>SELU dropout: alpha-scaled dropout for use with SELU activations. See: Klambauer et al. "Self-Normalizing Neural Networks", 2017. This makes the same default assumptions as our SELU activations. The paper recommends a default dropout rate of 0.05 (keep 0.95).</p>
<p>Implementation details: <a class="el" href="classlbann_1_1selu__dropout.html">lbann::selu_dropout</a></p>
<h2><a class="anchor" id="local_response_norm_layer"></a>
Local Response Norm Layer</h2>
<p>Local Response Normalization layer.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1local__response__normalization__layer.html">lbann::local_response_normalization_layer</a></p>
<h1><a class="anchor" id="transform"></a>
Transform</h1>
<h2><a class="anchor" id="concatenation"></a>
Concatenation</h2>
<p>Concatenation layer. This layer concatenates input tensors along a specified axis.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1concatenation__layer.html">lbann::concatenation_layer</a></p>
<h2><a class="anchor" id="noise"></a>
Noise</h2>
<p>Implementation details: lbann::noise_layer</p>
<h2><a class="anchor" id="unpooling"></a>
Unpooling</h2>
<p>Unpooling layer.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1unpooling__layer.html">lbann::unpooling_layer</a></p>
<h2><a class="anchor" id="pooling"></a>
Pooling</h2>
<p>Pooling layer.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1pooling__layer.html">lbann::pooling_layer</a></p>
<h2><a class="anchor" id="reshape"></a>
Reshape</h2>
<p>Reshape layer</p>
<p>Implementation details: <a class="el" href="classlbann_1_1pooling__layer.html">lbann::pooling_layer</a></p>
<h2><a class="anchor" id="slice"></a>
Slice</h2>
<p>Slice layer. This layer slices an input tensor along a specified axis.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1slice__layer.html">lbann::slice_layer</a></p>
<h2><a class="anchor" id="split"></a>
Split</h2>
<p>Split layer. This layer can accommodate an arbitrary number of outputs.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1split__layer.html">lbann::split_layer</a></p>
<h2><a class="anchor" id="sum"></a>
Sum</h2>
<p>Sum layer. This layer performs a weighted sum of input tensors, possibly with a different scaling factor for each input. If the scaling factors are not provided, they are all set to one so that this layer performs a simple sum.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1sum__layer.html">lbann::sum_layer</a></p>
<h1><a class="anchor" id="activation"></a>
Activation</h1>
<h2><a class="anchor" id="idlayer"></a>
Identity</h2>
<p>Implementation details: lbann::id_layer</p>
<h2><a class="anchor" id="reluLayer"></a>
Rectified Linear Unit</h2>
<p>Rectified linear unit activation function. See <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1relu__layer.html">lbann::relu_layer</a></p>
<h2><a class="anchor" id="leakyrelu"></a>
Leaky Relu</h2>
<p>Leaky rectified linear unit activation function. This is a ReLU variant that avoids the dying ReLU problem where a ReLU neuron can stop updating. See: Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. "Rectifier
nonlinearities improve neural network acoustic models." Proc. ICML. Vol. 30. No. 1. 2013.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1leaky__relu__layer.html">lbann::leaky_relu_layer</a>:</p>
<h2><a class="anchor" id="smoothrelu"></a>
Smooth Relu</h2>
<p>Smooth Rectified linear unit activation function. This is an approximation to the softplus.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1smooth__relu__layer.html">lbann::smooth_relu_layer</a></p>
<h2><a class="anchor" id="expLinUn"></a>
Exponential Linear Unit</h2>
<p>Exponential linear unit.</p>
<p>Tries to speed up learning by pushing the mean of activations more towards zero by allowing negative values. Helps avoid the need for batch normalization. See: Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)" ICLR 2016.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1elu__layer.html">lbann::elu_layer</a></p>
<h2><a class="anchor" id="seluLayer"></a>
Scaled Elu</h2>
<p>SELU: scaled exponential linear unit. See: Klambauer et al. "Self-Normalizing Neural Networks", 2017. <a href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</a> By default, this assumes the goal is to normalize to 0 mean/unit variance. To accomplish this, you should also normalize input to 0 mean/unit variance (z-score), initialize with 0 mean, 1/n variance (He), and use the SELU dropout.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1selu__layer.html">lbann::selu_layer</a></p>
<h2><a class="anchor" id="sigLayer"></a>
Sigmoid</h2>
<p>Sigmoid activation function. See <a href="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1sigmoid__layer.html">lbann::sigmoid_layer</a></p>
<h2><a class="anchor" id="softplus"></a>
Softplus</h2>
<p>Softplus activation function. This is a smooth approximation of the ReLU. See <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1softplus__layer.html">lbann::softplus_layer</a></p>
<h2><a class="anchor" id="softmax"></a>
Softmax</h2>
<p>Softmax layer.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1softmax__layer.html">lbann::softmax_layer</a></p>
<h2><a class="anchor" id="tanh"></a>
Tanh</h2>
<p>Hyperbolic tangent activation function.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1tanh__layer.html">lbann::tanh_layer</a></p>
<h2><a class="anchor" id="atan"></a>
Atan</h2>
<p>Arctangent activation function.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1atan__layer.html">lbann::atan_layer</a></p>
<h2><a class="anchor" id="bent_identity"></a>
Bent Identity</h2>
<p>Bent identity activation function. See <a href="https://en.wikipedia.org/wiki/Bent_Identity_function">https://en.wikipedia.org/wiki/Bent_Identity_function</a></p>
<p>Implementation details: <a class="el" href="classlbann_1_1bent__identity__layer.html">lbann::bent_identity_layer</a></p>
<h2><a class="anchor" id="exponential"></a>
Exponential</h2>
<p>Exponential activation function.</p>
<p>Implementation details: <a class="el" href="classlbann_1_1exponential__layer.html">lbann::exponential_layer</a></p>
<h1><a class="anchor" id="i_o"></a>
IO</h1>
<h2><a class="anchor" id="input"></a>
Input</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1input__layer.html">lbann::input_layer</a> </p>
<h2><a class="anchor" id="target"></a>
Target</h2>
<p>Implementation details: <a class="el" href="classlbann_1_1target__layer.html">lbann::target_layer</a> </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Tue Apr 24 2018 15:13:30 for LBANN by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
